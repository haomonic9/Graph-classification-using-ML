{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_dynamics_2.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYPanIEwk8yi"
      },
      "source": [
        "# load training data\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "er=np.load('dataset_example/dataset_er_symbolic.npy','r')\n",
        "sf=np.load('dataset_example/dataset_sf_symbolic.npy','r')\n",
        "sw=np.load('dataset_example/dataset_sw_symbolic.npy','r')\n",
        "##################### 3 classes #####################################\n",
        "er_label=np.zeros((np.shape(er)[0],3))\n",
        "er_label[:,0]=1\n",
        "sf_label=np.zeros((np.shape(sf)[0],3))\n",
        "sf_label[:,1]=1\n",
        "sw_label=np.zeros((np.shape(er)[0],3))\n",
        "sw_label[:,2]=1\n",
        "\n",
        "er_train, er_test, yer_train, yer_test = train_test_split(er, er_label, test_size=0.20, shuffle=True)\n",
        "sf_train, sf_test, ysf_train, ysf_test = train_test_split(sf, sf_label, test_size=0.20, shuffle=True)\n",
        "sw_train, sw_test, ysw_train, ysw_test = train_test_split(sw, sw_label, test_size=0.20, shuffle=True)\n",
        "\n",
        "x_train=np.concatenate((er_train,sf_train,sw_train),axis=0)\n",
        "x_test=np.concatenate((er_test,sf_test,sw_test),axis=0)\n",
        "y_train=np.concatenate((yer_train, ysf_train, ysw_train),axis=0)\n",
        "y_test=np.concatenate((yer_test,ysf_test,ysw_test),axis=0)\n",
        "\n",
        "x_train=np.reshape(x_train,(np.shape(x_train)[0],np.shape(x_train)[1],np.shape(x_train)[2],1))\n",
        "x_test=np.reshape(x_test,(np.shape(x_test)[0],np.shape(x_test)[1],np.shape(x_test)[2],1))\n",
        "\n",
        "print(np.shape(x_train),np.shape(x_test),np.shape(y_train),np.shape(y_test))\n",
        "print('train_Class1:',np.sum(y_train[:,0]),'train_Class2:',np.sum(y_train[:,1]),'train_Class3:',np.sum(y_train[:,2]))\n",
        "print('test_Class1:',np.sum(y_test[:,0]),'test_Class2:',np.sum(y_test[:,1]),'test_Class3:',np.sum(y_test[:,2]))\n",
        "\n",
        "# # ##################### 2 classes #####################################\n",
        "# er_label=np.zeros((np.shape(er)[0],3))\n",
        "# er_label[:,0]=1\n",
        "# sf_label=np.zeros((np.shape(sf)[0],3))\n",
        "# sf_label[:,1]=1\n",
        "\n",
        "# er_train, er_test, yer_train, yer_test = train_test_split(er, er_label, test_size=0.20, shuffle=True)\n",
        "# sf_train, sf_test, ysf_train, ysf_test = train_test_split(sf, sf_label, test_size=0.20, shuffle=True)\n",
        "\n",
        "# x_train=np.concatenate((er_train,sf_train),axis=0)\n",
        "# x_test=np.concatenate((er_test,sf_test),axis=0)\n",
        "# y_train=np.concatenate((yer_train,ysf_train),axis=0)\n",
        "# y_test=np.concatenate((yer_test,ysf_test),axis=0)\n",
        "\n",
        "# x_train=np.reshape(x_train,(np.shape(x_train)[0],np.shape(x_train)[1],np.shape(x_train)[2],1))\n",
        "# x_test=np.reshape(x_test,(np.shape(x_test)[0],np.shape(x_test)[1],np.shape(x_test)[2],1))\n",
        "\n",
        "# print(np.shape(x_train),np.shape(x_test),np.shape(y_train),np.shape(y_test))\n",
        "# print('train_Class1:',np.sum(y_train[:,0]),'train_Class2:',np.sum(y_train[:,1]))\n",
        "# print('test_Class1:',np.sum(y_test[:,0]),'test_Class2:',np.sum(y_test[:,1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjNvJ3--nqga"
      },
      "source": [
        "#Conv2D\n",
        "import tensorflow as tf\n",
        "# from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout,BatchNormalization, LeakyReLU, PReLU, Conv1D, Conv2D, GlobalMaxPooling1D, GlobalMaxPooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, MaxPooling1D, Flatten, Activation\n",
        "from tensorflow.keras.layers import GRU, LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "import os\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "MODEL_SAVE_FOLDER_PATH = './model_save_folder_path/'\n",
        "if not os.path.exists(MODEL_SAVE_FOLDER_PATH):\n",
        "    os.mkdir(MODEL_SAVE_FOLDER_PATH)\n",
        "\n",
        "length_list=[1,2,4,6,8,10,15,20,50,100,200] \n",
        "node_list=[1,2,4,6,8,10]\n",
        "\n",
        "total_error_result=np.zeros((len(length_list),len(node_list)))\n",
        "total_loss_result=np.zeros((len(length_list),len(node_list)))\n",
        "\n",
        "for j in range(len(node_list)):\n",
        "    for k in range(len(length_list)):\n",
        "\n",
        "        model_path = MODEL_SAVE_FOLDER_PATH + 'model_er_sf_length{a}_node{b}_sym.hdf5'.format(a=length_list[k],b=node_list[j])\n",
        "\n",
        "        cb_checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss',\n",
        "                                        verbose=0, save_best_only=True)\n",
        "\n",
        "        cb_early_stopping = EarlyStopping(monitor='val_loss', patience=60)\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Conv2D(80,\n",
        "                        (3,2),\n",
        "                        input_shape=(length_list[k], node_list[j],1),\n",
        "                        padding='same',\n",
        "                        activation='linear',\n",
        "                        strides=1,\n",
        "                        data_format=\"channels_last\")) #2048, 4\n",
        "        model.add(Activation('selu'))\n",
        "        model.add(Conv2D(80,\n",
        "                        (3,2),\n",
        "                        padding='same',\n",
        "                        activation='linear',\n",
        "                        strides=1)) #2048, 4 \n",
        "        model.add(Activation('selu'))\n",
        "        model.add(Conv2D(40,\n",
        "                        (3,2),\n",
        "                        padding='same',\n",
        "                        activation='linear',\n",
        "                        strides=1)) #2048, 4\n",
        "        model.add(Activation('selu'))\n",
        "        model.add(GlobalAveragePooling2D())\n",
        "        model.add(Dense(80, activation='selu'))\n",
        "        # model.add(Dropout(0.3))\n",
        "        model.add(Dense(80, activation='selu'))\n",
        "        # model.add(Dropout(0.3))\n",
        "        model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "        adam = tf.keras.optimizers.Adam(lr=0.0005)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=adam,\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "        print('###########################Training start(length{a}, node{b})#############################'.format(a=length_list[k],b=node_list[j]))\n",
        "        model.summary()\n",
        "\n",
        "        history=model.fit(x_train[:,0:length_list[k],0:node_list[j],:], y_train[:,:], validation_data=(x_test[:,0:length_list[k],0:node_list[j],:],y_test),epochs=500, batch_size=10, verbose=1 ,shuffle = True, callbacks=[cb_early_stopping,cb_checkpoint])\n",
        "\n",
        "        model = load_model(model_path) #load best model\n",
        "\n",
        "        # training_acc=model.evaluate(x_train[0:sample_list[l],0:length_list[k],0:node_list[j],:], y_train[0:sample_list[l],:])[1]\n",
        "        # training_loss=model.evaluate(x_train[0:sample_list[l],0:length_list[k],0:node_list[j],:], y_train[0:sample_list[l],:])[0]\n",
        "        test_acc=model.evaluate(x_test[:,0:length_list[k],0:node_list[j],:],y_test[0:sample_list[l],:])[1]\n",
        "        test_loss=model.evaluate(x_test[:,0:length_list[k],0:node_list[j],:],y_test[0:sample_list[l],:])[0]\n",
        "        # total_acc=model.evaluate(input_data[:,0:length_list[k],0:node_list[j],:], output_data)[1]\n",
        "        # total_loss=model.evaluate(input_data[:,0:length_list[k],0:node_list[j],:], output_data)[0]\n",
        "\n",
        "        total_error_result[k,j]=1-test_acc\n",
        "        total_loss_result[k,j]=test_loss\n",
        "\n",
        "\n",
        "\n",
        "        np.savetxt('error_sweep.txt', total_error_result)           \n",
        "        np.savetxt('loss_sweep.txt', total_loss_result) \n",
        "np.savetxt('error_sweep.txt', total_error_result)           \n",
        "np.savetxt('loss_sweep.txt', total_loss_result) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiEmr0fC0JXR"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sweep_result=np.loadtxt('error_sweep.txt')\n",
        "\n",
        "yticks=length_list.copy()\n",
        "xticks=node_list.copy()\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "im = ax.imshow(sweep_result[:,:],aspect='auto')\n",
        "\n",
        "ax.set_xticks(np.arange(len(xticks)))\n",
        "ax.set_yticks(np.arange(len(yticks)))\n",
        "# ax.set_xticklabels(xticks,size=30,rotation = 90)\n",
        "ax.set_xticklabels(xticks,size=30)\n",
        "ax.set_yticklabels(yticks,size=30)\n",
        "\n",
        "ax.set_xlabel('n',size=40)\n",
        "ax.set_ylabel('t', size=40)\n",
        "clb=fig.colorbar(im, ax=ax, orientation='vertical')\n",
        "clb.mappable.set_clim(0,0.5)\n",
        "clb.ax.tick_params(labelsize=30)\n",
        "############################################################################\n",
        "# plt.title(r'$N=100$ $\\langle k \\rangle =20$ $\\lambda = 0.03$', fontsize=20)\n",
        "# plt.title(r'$\\rangle$', fontsize=20)\n",
        "###########################################################################\n",
        "clb.ax.set_title('Error rate', fontsize=30,pad=20)\n",
        "for (j,i),label in np.ndenumerate(sweep_result):\n",
        "    plt.text(i,j,round(label,2),ha='center',va='center',fontsize = 20, color='w')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}